{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lbw/miniconda3/envs/CLODE_128_bw/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from network.conv_node import NODE\n",
    "from misc import *\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.multimodal import CLIPImageQualityAssessment\n",
    "import time\n",
    "\n",
    "# GPU 번호 지정\n",
    "gpu_number = 3  # 원하는 GPU 번호로 변경 가능\n",
    "\n",
    "# GPU 사용 제한\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_number)\n",
    "\n",
    "# 이제 GPU가 한 개만 보이므로 cuda:0으로 접근\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model and Dataset (LOL train 485 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = NODE(device, (3, 256, 256), 32, augment_dim=0, time_dependent=True, adjoint=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(f'/home/lbw/CLODE/pth/universal.pth', weights_only=True), strict=False)\n",
    "\n",
    "file_path = Path('/home/lbw/data/our485')\n",
    "# img_labels = sorted(os.listdir(file_path / 'low'))\n",
    "img_labels = [f for f in sorted(os.listdir(file_path / 'low')) if f.lower().endswith('.png')]\n",
    "\n",
    "def load_image(idx):\n",
    "    lq_img = image_tensor(file_path / 'low' / img_labels[idx], size=(256, 256))\n",
    "    gt_img = image_tensor(file_path / 'high' / img_labels[idx], size=(256, 256))\n",
    "    \n",
    "    return lq_img.to(device), gt_img.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.22564435005187988\n",
      "Brightness: 0.19360555708408356, Noisiness: 0.3640592396259308, Quality: 0.5380584001541138\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompts = ('brightness', 'noisiness', 'quality')  # 단순 튜플로 변경\n",
    "\n",
    "clip_metric = CLIPImageQualityAssessment(\n",
    "    model_name_or_path=\"openai/clip-vit-base-patch16\",\n",
    "    prompts=prompts  # 이미 적절한 형식을 가진 튜플\n",
    ").to(device)\n",
    "\n",
    "def calculate_clip_score(pred, prompts=prompts):    \n",
    "    # 이미 배치 차원이 있는지 확인하고 없으면 추가\n",
    "    if len(pred.shape) == 3:\n",
    "        pred = pred.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 한 번의 forward pass로 모든 프롬프트에 대한 점수를 계산\n",
    "        scores = clip_metric(pred)\n",
    "\n",
    "    # 결과 반환 (scores는 리스트 형태로 반환됨)\n",
    "    return scores[prompts[0]].item(), scores[prompts[1]].item(), scores[prompts[2]].item()\n",
    "\n",
    "# 테스트\n",
    "start = time.time()\n",
    "a,b,c = calculate_clip_score(torch.rand(3, 256, 256).to(device))\n",
    "print(f\"Time: {time.time() - start}\")\n",
    "print(f\"Brightness: {a}, Noisiness: {b}, Quality: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try find best T values by PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/485 [02:54<4:29:34, 33.70s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "# 메인 루프 최적화\n",
    "T_values = np.linspace(2, 5, 30)\n",
    "\n",
    "results = []\n",
    "brightness_scores = []\n",
    "noisiness_scores = []\n",
    "quality_scores = []\n",
    "\n",
    "# T 값들을 먼저 텐서로 변환하여 반복 변환 방지\n",
    "T_tensors = [torch.tensor([0, T]).float().cuda() for T in T_values]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in tqdm(range(len(img_labels))):\n",
    "        lq_img, gt_img = load_image(idx)\n",
    "        high_psnr = 0.0\n",
    "        best_T = 2.0\n",
    "        \n",
    "        # 모든 T에 대한 예측을 한 번에 계산\n",
    "        preds = []\n",
    "        psnrs = []\n",
    "        \n",
    "        for i, T_tensor in enumerate(T_tensors):\n",
    "            pred = model(lq_img, T_tensor, inference=True)['output'][0]\n",
    "            preds.append(pred)\n",
    "            psnr = calculate_psnr(pred, gt_img).item()\n",
    "            psnrs.append(psnr)\n",
    "            \n",
    "            if high_psnr < psnr:\n",
    "                high_psnr = psnr\n",
    "                best_T = T_values[i]\n",
    "        \n",
    "        # 모든 예측에 대해 CLIP 점수 계산\n",
    "        for i, pred in enumerate(preds):\n",
    "            bright_score, noise_score, quality_score = calculate_clip_score(pred)\n",
    "            brightness_scores.append([idx, T_values[i], bright_score])\n",
    "            noisiness_scores.append([idx, T_values[i], noise_score])\n",
    "            quality_scores.append([idx, T_values[i], quality_score])\n",
    "        \n",
    "        results.append([best_T, high_psnr])\n",
    "\n",
    "save_path = Path('/home/lbw/CLODE/scores_csv')\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "results = np.array(results)\n",
    "np.save(Path(save_path / 'results.npy'), results)\n",
    "\n",
    "brightness_scores = np.array(brightness_scores)\n",
    "noisiness_scores = np.array(noisiness_scores)\n",
    "quality_scores = np.array(quality_scores)\n",
    "\n",
    "np.save(Path(save_path / 'brightness_scores.npy'), brightness_scores)\n",
    "np.save(Path(save_path / 'noisiness_scores.npy'), noisiness_scores)\n",
    "np.save(Path(save_path / 'quality_scores.npy'), quality_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt $\\alpha$, $\\beta$, $\\gamma$ so that weighted IQA score approximates score with optimal T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['brightness', 'noisiness', 'quality']\n",
    "weights = [1.0, 1.0, 1.0]\n",
    "clip_iqa = CLIPImageQualityAssessment(prompts=prompts).to(device)\n",
    "\n",
    "learning_rate = 0.1\n",
    "asc, desc = learning_rate * len(prompts) / (len(prompts) -1), learning_rate / (len(prompts) - 1)\n",
    "\n",
    "def adjust_clip_weights(pred, weights):\n",
    "    score = clip_iqa(pred.unsqueeze(0))\n",
    "    scores = [score[prompt].item() for prompt in prompts]\n",
    "    max_idx = np.argmax(scores)\n",
    "    \n",
    "    weights[max_idx] += asc\n",
    "    weights -= desc\n",
    "    \n",
    "    return weights\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in tqdm(range(len(img_labels))):\n",
    "        lq_img, gt_img = load_image(idx)\n",
    "\n",
    "        T = results[idx][0]\n",
    "        integration_time = torch.tensor([0, T]).float().cuda()\n",
    "        pred = model(lq_img, integration_time, inference=True)['output'][0]\n",
    "            \n",
    "        weights = adjust_clip_weights(pred, weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLODE_128_bw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
