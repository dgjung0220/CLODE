{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Regression (3 CLIP scores, and T -> predict best T) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import random\n",
    "\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lbw/miniconda3/envs/CLODE_128_bw/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics.multimodal import CLIPImageQualityAssessment\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "# Regressor 모델 (입력: (batch_size, 4, 768))\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, vision_dim=512, text_dim=512, hidden_dim=64, num_prompts=3):\n",
    "        super(Regressor, self).__init__()\n",
    "        self.num_prompts = num_prompts\n",
    "        self.vision_mlp = nn.Sequential(\n",
    "            nn.Linear(vision_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, combined_input):\n",
    "        # combined_input: (batch_size, 4, 768)\n",
    "        batch_size = combined_input.size(0)\n",
    "\n",
    "        \n",
    "        # Vision MLP: (batch_size, 4, 768) -> (B, 4, 64)\n",
    "        combined_input = self.vision_mlp(combined_input)  # (B, 4, 64)\n",
    "        \n",
    "        \n",
    "        # Cross-Attention\n",
    "        vision_cls = combined_input[:, 0, :].unsqueeze(dim=1) # (batch_size, 1, 64)\n",
    "        text_feature = combined_input[:, 1:self.num_prompts+1, :]  # (batch_size, 3, 64)\n",
    "        # print(vision_cls.shape, text_feature.shape)\n",
    "\n",
    "        vision_cls = vision_cls.permute(1, 0, 2)  # (1, B, 64)\n",
    "        text_feature = text_feature.permute(1, 0, 2)  # (3, B, 64)\n",
    "        attn_output, a = self.cross_attention(vision_cls, text_feature, text_feature)  # (B, 1, 64)\n",
    "        attn_output = attn_output.permute(1, 0, 2)  # (B, 1, 64)\n",
    "        \n",
    "        # FC Layer\n",
    "        T_pred = self.fc(attn_output.squeeze(1)).squeeze(-1)  # (B,)\n",
    "        return T_pred\n",
    "\n",
    "# 테스트\n",
    "if __name__ == \"__main__\":\n",
    "    x = torch.randn(2, 4, 512)  # 예시 텍스트 특징\n",
    "    model = Regressor()\n",
    "    output = model(x)\n",
    "    print(\"Output shape:\", output.shape)  # torch.Size([2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test할 때 T 없는 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lbw/CLODE/regression_pth/lol_soom_cls_text_ca2_64_pca/att_regression_168.pth 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1961698/139869171.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  regressor.load_state_dict(torch.load(model_path_name, map_location=device))\n",
      "100%|██████████| 30/30 [05:16<00:00, 10.56s/it]\n",
      "100%|██████████| 20/20 [02:55<00:00,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 이미지 처리 완료!\n",
      "Predicted T PSNR 평균: 18.0804dB\n",
      "Predicted T SSIM 평균: 0.5366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from network.conv_node import NODE\n",
    "# from network.clip_classifier import TtoTClassifier\n",
    "from misc import *\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.multimodal import CLIPImageQualityAssessment\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# GPU 번호 지정\n",
    "gpu_number = 2  # 원하는 GPU 번호로 변경 가능\n",
    "\n",
    "# GPU 사용 제한\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_number)\n",
    "\n",
    "# 이제 GPU가 한 개만 보이므로 cuda:0으로 접근\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = NODE(device, (3, 400, 600), 32, augment_dim=0, time_dependent=True, adjoint=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(f'/home/lbw/CLODE/pth/lowlight.pth', weights_only=True), strict=False)\n",
    "\n",
    "# 결과 저장을 위한 디렉토리 생성\n",
    "results_dir = Path('/home/lbw/CLODE/result_img/cls_text_ca_pca')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "input_dim = 768\n",
    "hidden_dim = 64\n",
    "# score_dim = len(prompts)\n",
    "\n",
    "# 모델 초기화\n",
    "regressor = Regressor(hidden_dim=hidden_dim).to(device)\n",
    "regressor.eval()\n",
    "regressor.to(device)\n",
    "# model_path_name = (model_path / f'att_regression_{best_epoch}.pth')\n",
    "# model_path_name = '/home/lbw/CLODE/regression_pth/lbw_low_feature_L_score_model2/att_regression_27.pth'\n",
    "# model_path_name = '/home/lbw/CLODE/regression_pth/lol_T_sa_ca_model/att_regression_17.pth'\n",
    "# model_path_name = '/home/lbw/CLODE/regression_pth/lol_T_sa_ca_model_soom/att_regression_42.pth'\n",
    "model_path_name = '/home/lbw/CLODE/regression_pth/lol_soom_cls_text_ca2_64_pca/att_regression_168.pth'\n",
    "print(model_path_name, input_dim)\n",
    "regressor.load_state_dict(torch.load(model_path_name, map_location=device))\n",
    "\n",
    "\n",
    "\n",
    "# Text embedding 로드\n",
    "text_embedding = torch.tensor(np.load('/home/lbw/CLODE/traindata_csv/text_embedding_pca.npy'), dtype=torch.float32).to(device)  # (3, 768)\n",
    "text_embedding_un = text_embedding.unsqueeze(0).to(device)  # (1, 3, 768)\n",
    "\n",
    "# CLIP-IQA 로드 (eval15 데이터의 vision_cls 계산용)\n",
    "clip_iqa = CLIPImageQualityAssessment(\n",
    "    model_name_or_path=\"openai/clip-vit-base-patch16\",\n",
    "    prompts=('brightness', 'natural', 'colorfullness')\n",
    ").to(device)\n",
    "\n",
    "# Transform 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                        std=[0.26862954, 0.26130258, 0.27577711])\n",
    "])\n",
    "\n",
    "# eval15 데이터 불러오기\n",
    "\n",
    "base_eval_path = Path('/home/lbw/data/LSRW/Eval/')\n",
    "camera_types = ['Huawei', 'Nikon']\n",
    "\n",
    "def load_eval_image(idx):\n",
    "    img_name = eval_images[idx]\n",
    "    lq_img = image_tensor(eval_path / 'low' / img_name)\n",
    "    gt_img = image_tensor(eval_path / 'high' / img_name)\n",
    "    \n",
    "    # lq_img_224는 transform 적용\n",
    "    lq_img_pil = Image.open(eval_path / 'low' / img_name).convert('RGB')\n",
    "    lq_img_224 = transform(lq_img_pil).unsqueeze(0).to(device)  # (1, 3, 224, 224)\n",
    "    \n",
    "    # Vision CLS 계산\n",
    "    with torch.no_grad():\n",
    "        vision_feature = clip_iqa.model.vision_model(lq_img_224)[1]  # (1, 768)\n",
    "        vision_cls = clip_iqa.model.visual_projection(vision_feature)  # (1, 768)\n",
    "    \n",
    "    return lq_img.to(device), gt_img.to(device), img_name, vision_cls.to(device)\n",
    "\n",
    "def tensor_to_numpy(tensor):\n",
    "    img = tensor.detach().cpu().numpy()\n",
    "    if img.ndim == 3 and img.shape[0] == 3:\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "pred_img = []\n",
    "pred_Ts = []\n",
    "pred_psnrs = []\n",
    "pred_ssims = []\n",
    "\n",
    "# 이미지별 NODE와 Classifier 결과 비교\n",
    "# with alive_bar(len(eval_images), title='Processing images', bar='notes', spinner='waves', force_tty=True, monitor=True) as bar:\n",
    "for camera_type in camera_types:\n",
    "    eval_path = base_eval_path / camera_type\n",
    "    eval_images = [f for f in sorted(os.listdir(eval_path / 'low')) if f.lower().endswith('.jpg')]\n",
    "\n",
    "\n",
    "    # 이미지별 NODE와 Regressor 결과 비교\n",
    "    for idx in tqdm(range(len(eval_images))):\n",
    "        lq_img, gt_img, img_name, vision_cls = load_eval_image(idx)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # vision_cls: (1, 768)\n",
    "            vision_cls = vision_cls.unsqueeze(1)  # (1, 1, 768)\n",
    "            text_batch = text_embedding_un.repeat(vision_cls.size(0), 1, 1).to(device)  # (1, 3, 768)\n",
    "            combined_input = torch.cat([vision_cls, text_batch], dim=1)  # (1, 4, 768)\n",
    "            \n",
    "            # Regressor로 T 예측\n",
    "            pred_T = regressor(combined_input)\n",
    "            pred_T = pred_T.item()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            T_tensor = torch.tensor([0, pred_T]).float().to(device)\n",
    "            pred = model(lq_img, T_tensor, inference=True)['output'][0]\n",
    "            psnr = calculate_psnr(pred, gt_img).item()\n",
    "            ssim_value = calculate_ssim(pred, gt_img)\n",
    "\n",
    "        pred_img.append(pred)    \n",
    "        pred_Ts.append(pred_T)\n",
    "        pred_psnrs.append(psnr)\n",
    "        pred_ssims.append(ssim_value)\n",
    "        \n",
    "        # 시각화\n",
    "        plt.figure(figsize=(18, 6))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(tensor_to_numpy(lq_img[0]))\n",
    "        plt.title(f'Low Quality Image: {img_name}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(tensor_to_numpy(gt_img[0]))\n",
    "        plt.title('Ground Truth')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(tensor_to_numpy(pred))\n",
    "        plt.title(f'Regressor Pred T={pred_T:.2f}, PSNR={psnr:.4f}dB, SSIM={ssim_value:.4f}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(results_dir / f'{img_name.split(\".\")[0]}_compare.png')\n",
    "        plt.close()\n",
    "\n",
    "# 통계 정보 계산 (전체 이미지에 대해)\n",
    "print(\"모든 이미지 처리 완료!\")\n",
    "\n",
    "# PSNR 비교\n",
    "# print(f\"Best T PSNR 평균: {np.mean(best_psnrs):.2f}dB\")\n",
    "print(f\"Predicted T PSNR 평균: {np.mean(pred_psnrs):.4f}dB\")\n",
    "# print(f\"Best T SSIM 평균: {np.mean(best_ssims):.4f}\")\n",
    "print(f\"Predicted T SSIM 평균: {np.mean(pred_ssims):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lbw/miniconda3/envs/CLODE_128_bw/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP embeddings saved: text_embedding.npy, vision_cls.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchmetrics.multimodal import CLIPImageQualityAssessment\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "# GPU 번호 지정\n",
    "gpu_number = 2  # 원하는 GPU 번호로 변경 가능\n",
    "\n",
    "# GPU 사용 제한\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_number)\n",
    "\n",
    "# 이제 GPU가 한 개만 보이므로 cuda:0으로 접근\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# CLIP-IQA 로드\n",
    "clip_iqa = CLIPImageQualityAssessment(\n",
    "    model_name_or_path=\"openai/clip-vit-base-patch16\",\n",
    "    prompts=('brightness', 'natural', 'colorfullness')\n",
    ").to(device)\n",
    "\n",
    "# CLIP 토크나이저 로드\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# 이미지 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                        std=[0.26862954, 0.26130258, 0.27577711])\n",
    "])\n",
    "\n",
    "# 이미지 로드\n",
    "file_path = Path('/home/lbw/data/LOL/our485')\n",
    "img_labels = [f for f in sorted(os.listdir(file_path / 'low')) if f.lower().endswith('.png')]\n",
    "\n",
    "# 1. Text Embedding 계산 및 저장\n",
    "prompts = ('brightness', 'natural', 'colorfullness')\n",
    "tokenized = tokenizer(\n",
    "    prompts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=77\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_feature = clip_iqa.model.text_model(\n",
    "        input_ids=tokenized['input_ids'].to(device),\n",
    "        attention_mask=tokenized['attention_mask'].to(device)\n",
    "    )  # (num_prompts, 768)\n",
    "    text_feature = clip_iqa.model.text_projection(text_feature[1])  # (num_prompts, 768)\n",
    "\n",
    "# numpy 배열로 변환 및 저장\n",
    "text_embedding = text_feature.cpu().numpy()  # (3, 768)\n",
    "np.save('/home/lbw/CLODE/traindata_csv/text_embedding_pca.npy', text_embedding)\n",
    "\n",
    "# 2. Vision CLS 계산 및 저장\n",
    "vision_cls_list = []\n",
    "for img_name in img_labels:\n",
    "    img_path = file_path / 'low' / img_name\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)  # (1, 3, 224, 224)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vision_feature = clip_iqa.model.vision_model(image)[1]  # (1, 768)\n",
    "        vision_cls = clip_iqa.model.visual_projection(vision_feature)  # (1, 768)\n",
    "    \n",
    "    vision_cls_list.append(vision_cls.cpu())\n",
    "\n",
    "# 리스트를 텐서로 결합 후 numpy 배열로 변환 및 저장\n",
    "vision_cls = torch.cat(vision_cls_list, dim=0)  # (485, 768)\n",
    "np.save('/home/lbw/CLODE/traindata_csv/vision_cls_pca.npy', vision_cls.numpy())\n",
    "print(\"CLIP embeddings saved: text_embedding.npy, vision_cls.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLODE_128_bw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
