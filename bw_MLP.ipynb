{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class TtoTClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        입력: (T, N) 형태의 데이터\n",
    "        출력: (T,) 형태의 스칼라 값들\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): 입력 특성의 차원 (N)\n",
    "            hidden_dim (int): 은닉층 차원\n",
    "        \"\"\"\n",
    "        super(TtoTClassifier, self).__init__()\n",
    "        \n",
    "        # 각 시점(행)을 독립적으로 처리하는 신경망\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # 각 시점마다 하나의 스칼라 값 출력\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 입력 텐서 [batch_size, T, N] 또는 [T, N]\n",
    "            \n",
    "        Returns:\n",
    "            출력 텐서 [batch_size, T] 또는 [T]\n",
    "        \"\"\"\n",
    "        # 입력이 2D인지 3D인지 확인 (배치 차원 유무)\n",
    "        original_shape = x.shape\n",
    "        if len(original_shape) == 2:  # [T, N]\n",
    "            x = x.unsqueeze(0)  # [1, T, N]\n",
    "\n",
    "        batch_size, seq_len, feature_dim = x.shape\n",
    "        \n",
    "        # 모든 시점을 하나의 큰 배치로 처리\n",
    "        x_reshaped = x.reshape(-1, feature_dim)  # [batch_size*T, N]\n",
    "        \n",
    "        # 각 시점에 대한 예측 (스칼라 값)\n",
    "        out_flat = self.net(x_reshaped)  # [batch_size*T, 1]\n",
    "        \n",
    "        # 원래 형태로 복원\n",
    "        out = out_flat.reshape(batch_size, seq_len)  # [batch_size, T]\n",
    "        \n",
    "        # 원본 입력이 2D였다면 squeeze\n",
    "        if len(original_shape) == 2:\n",
    "            out = out.squeeze(0)  # [T]\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def classify(self, x, temperature=1.0):\n",
    "        \"\"\"\n",
    "        회귀 출력에 softmax를 적용하여 분류 확률로 변환\n",
    "        \n",
    "        Args:\n",
    "            x: 입력 텐서\n",
    "            temperature: softmax 온도 매개변수 (낮을수록 더 날카로운 분포)\n",
    "            \n",
    "        Returns:\n",
    "            각 시점에 대한 확률 분포\n",
    "        \"\"\"\n",
    "        logits = self.forward(x)\n",
    "        \n",
    "        # softmax 적용\n",
    "        if len(logits.shape) == 1:  # [T]\n",
    "            probs = F.softmax(logits / temperature, dim=0)\n",
    "        else:  # [batch_size, T]\n",
    "            probs = F.softmax(logits / temperature, dim=1)\n",
    "            \n",
    "        return probs\n",
    "    \n",
    "# 모델 학습 코드\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "T = 30  # 시퀀스 길이 (예: T 값의 개수)\n",
    "N = 3 # 각 시점의 특성 차원 (예: 이미지 특성)\n",
    "hidden_dim = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "# batch_size = 32\n",
    "\n",
    "# 모델 초기화\n",
    "model = TtoTClassifier(N, hidden_dim)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = nn.MSELoss()  # 회귀 손실\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 데이터 준비\n",
    "sample_input = torch.rand(T, N).to(device)\n",
    "# sample_target = torch.rand(T).to(device)\n",
    "\n",
    "sample_pred = model(sample_input)\n",
    "sample_class = sample_pred.argmax(dim=0)\n",
    "\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {sample_pred.shape}\")\n",
    "print(f\"Output class: {sample_class}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
